{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C4_W1_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdEnmVvXdUa"
      },
      "source": [
        "# Train Your Own Model and Serve It With TensorFlow Serving\n",
        "\n",
        "In this notebook, you will train a neural network to classify images of handwritten digits from the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. You will then save the trained model, and serve it using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfb9Qd5XdFL"
      },
      "source": [
        "**Warning: This notebook is designed to be run in a Google Colab only**.  It installs packages on the system and requires root access. If you want to run it in a local Jupyter notebook, please proceed with caution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Q8bkjeYTl-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8r89tTPI-Kb"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGFJmWjrKttn",
        "outputId": "5ac81b8e-83f7-4061-ea50-113c84c71330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â€¢ Using TensorFlow Version: 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq-214o8SNt0"
      },
      "source": [
        "## Import the MNIST Dataset\n",
        "\n",
        "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains 70,000 grayscale images of the digits 0 through 9. The images show individual digits at a low resolution (28 by 28 pixels). \n",
        "\n",
        "Even though these are really images, we will load them as NumPy arrays and not as binary image objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqDQO0KCaWS",
        "outputId": "59bb956a-8f82-42f6-832d-367802748ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIT-qX0QzLo-"
      },
      "source": [
        "# EXERCISE: Scale the values of the arrays below to be between 0.0 and 1.0.\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIDGu-EEzdKb"
      },
      "source": [
        "In the cell below use the `.reshape` method to resize the arrays to the following sizes:\n",
        "\n",
        "```python\n",
        "train_images.shape: (60000, 28, 28, 1)\n",
        "test_images.shape: (10000, 28, 28, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsIxeG6BzN4t"
      },
      "source": [
        "# EXERCISE: Reshape the arrays below.\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUw8ZxigB1Nx",
        "outputId": "eb49d62b-5be8-4e50-b27d-8c88d173c16c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
        "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train_images.shape: (60000, 28, 28, 1), of float64\n",
            "test_images.shape: (10000, 28, 28, 1), of float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcR0OKbOSj0c"
      },
      "source": [
        "## Look at a Sample Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMs4v_oSo9v",
        "outputId": "608802c4-5067-45cc-f53d-8e175bc66141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "idx = 42\n",
        "\n",
        "plt.imshow(test_images[idx].reshape(28,28), cmap=plt.cm.binary)\n",
        "plt.title('True Label: {}'.format(test_labels[idx]), fontdict={'size': 16})\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn_-9OsPYnDp"
      },
      "source": [
        "## Build a Model\n",
        "\n",
        "In the cell below build a `tf.keras.Sequential` model that can be used to classify the images of the MNIST dataset. Feel free to use the simplest possible CNN. Make sure your model has the correct `input_shape` and the correct number of output units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgMgJJynMbVY",
        "outputId": "9db157da-0050-47e5-b63f-7f031f7f5ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Create a model.\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(input_shape=(28, 28, 1),filters=8, kernel_size=3, strides=2, activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 13, 13, 8)         80        \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1352)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                13530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,610\n",
            "Trainable params: 13,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLzXnZT1YvS6"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "In the cell below configure your model for training using the `adam` optimizer, `sparse_categorical_crossentropy` as the loss, and `accuracy` for your metrics. Then train the model for the given number of epochs, using the `train_images` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTNN0ANGgA36",
        "outputId": "1cf6c0e8-f07c-42ee-caf2-37c9000901a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Configure the model for training.\n",
        "model.compile(\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    optimizer = 'adam',\n",
        "    metrics = ['acc']\n",
        ")\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# EXERCISE: Train the model.\n",
        "history = model.fit(train_images, train_labels, epochs=epochs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 18s 4ms/step - loss: 0.3305 - acc: 0.9088\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1579 - acc: 0.9561\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1246 - acc: 0.9645\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1078 - acc: 0.9689\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0977 - acc: 0.9716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er_vrDhf4qu5"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMD387B93f2g",
        "outputId": "13325759-900c-42d4-a311-bca7f03cdc55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Evaluate the model on the test images.\n",
        "results_eval = model.evaluate(test_images, test_labels)\n",
        "\n",
        "for metric, value in zip(model.metrics_names, results_eval):\n",
        "    print(metric + ': {:.3}'.format(value))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0934 - acc: 0.9728\n",
            "loss: 0.0934\n",
            "acc: 0.973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGfmT8M1Yx5y"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uFDoDW_7HX6",
        "outputId": "82e83b2a-f4d1-4d65-98bb-a062a573b88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MODEL_DIR = tempfile.gettempdir()\n",
        "\n",
        "version = 1\n",
        "\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "\n",
        "if os.path.isdir(export_path):\n",
        "    print('\\nAlready saved a model, cleaning up\\n')\n",
        "    !rm -r {export_path}\n",
        "\n",
        "model.save(export_path, save_format=\"tf\")\n",
        "\n",
        "print('\\nexport_path = {}'.format(export_path))\n",
        "!ls -l {export_path}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/1/assets\n",
            "\n",
            "export_path = /tmp/1\n",
            "total 88\n",
            "drwxr-xr-x 2 root root  4096 Apr  2 21:40 assets\n",
            "-rw-r--r-- 1 root root  8090 Apr  2 21:40 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 73101 Apr  2 21:40 saved_model.pb\n",
            "drwxr-xr-x 2 root root  4096 Apr  2 21:40 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KziE3e9tY-hH"
      },
      "source": [
        "## Examine Your Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU4GDF_aYtfQ",
        "outputId": "c2908c45-ecc2-475e-82bc-4359707b2873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['conv2d_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_conv2d_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "\n",
            "Concrete Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDTdBGHZAzo"
      },
      "source": [
        "## Add TensorFlow Serving Distribution URI as a Package Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWg9X2QHlbGS",
        "outputId": "cfff5e90-b588-4dbe-b388-cad63e0d50d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2943  100  2943    0     0   124k      0 --:--:-- --:--:-- --:--:--  124k\n",
            "OK\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:11 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [341 B]\n",
            "Get:12 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [349 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [80.8 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [950 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,830 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [884 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,132 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,693 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [937 kB]\n",
            "Get:27 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,486 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [918 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,264 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [12.9 kB]\n",
            "Get:32 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:33 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.3 kB]\n",
            "Fetched 15.6 MB in 4s (4,191 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "82 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l5XkzqNZNBU"
      },
      "source": [
        "## Install TensorFlow Serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwa9AgRloYy",
        "outputId": "1356ca16-ba0b-4dc5-f8c5-77c0580e8070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tensorflow-model-server\n",
            "0 upgraded, 1 newly installed, 0 to remove and 82 not upgraded.\n",
            "Need to get 340 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.8.0 [340 MB]\n",
            "Fetched 340 MB in 5s (73.2 MB/s)\n",
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.8.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.8.0) ...\n",
            "Setting up tensorflow-model-server (2.8.0) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd_PobAKZWW8"
      },
      "source": [
        "## Run the TensorFlow Model Server\n",
        "\n",
        "You will now launch the TensorFlow model server with a bash script. In the cell below use the following parameters when running the TensorFlow model server:\n",
        "\n",
        "* `rest_api_port`: Use port `8501` for your requests.\n",
        "\n",
        "\n",
        "* `model_name`: Use `digits_model` as your model name. \n",
        "\n",
        "\n",
        "* `model_base_path`: Use the environment variable `MODEL_DIR` defined below as the base path to the saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUgp3vUdU5GS"
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJDhHNJVnaLN",
        "outputId": "3d384ed5-642b-49af-83e3-6b52dabad6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Fill in the missing code below.\n",
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=digits_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxbeiOCUUs2z",
        "outputId": "bd084696-9de4-4746-8719-b104d7da1602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUrIWVRZdNu"
      },
      "source": [
        "## Create JSON Object with Test Images\n",
        "\n",
        "In the cell below construct a JSON object and use the first three images of the testing set (`test_images`) as your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dsD7KQG1m-R"
      },
      "source": [
        "# EXERCISE: Create JSON Object\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRdyPl4CZ5CU"
      },
      "source": [
        "## Make Inference Request\n",
        "\n",
        "In the cell below, send a predict request as a POST to the server's REST endpoint, and pass it your test data. You should ask the server to give you the latest version of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvFyuIzW6n6"
      },
      "source": [
        "# EXERCISE: Fill in the code below\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/digits_model:predict', data=data, headers=headers)\n",
        "    \n",
        "predictions = json.loads(json_response.text)['predictions']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrFMts_ackX"
      },
      "source": [
        "## Plot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxQzj34aiDz1",
        "outputId": "f388d9a2-735d-4464-9a96-11536e4ed977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "plt.figure(figsize=(10,15))\n",
        "\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(test_images[i].reshape(28,28), cmap = plt.cm.binary)\n",
        "    plt.axis('off')\n",
        "    color = 'green' if np.argmax(predictions[i]) == test_labels[i] else 'red'\n",
        "    plt.title('Prediction: {}\\nTrue Label: {}'.format(np.argmax(predictions[i]), test_labels[i]), color=color)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x1080 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}